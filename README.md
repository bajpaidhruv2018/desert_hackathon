<div align="center">

# ğŸœï¸ OFF-ROAD AUTONOMOUS VISION
### Robust Semantic Segmentation for Desert Terrains

<br>

![Python](https://img.shields.io/badge/Python-3.10-3776AB?style=for-the-badge&logo=python&logoColor=white)
![PyTorch](https://img.shields.io/badge/PyTorch-2.x-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white)
![CUDA](https://img.shields.io/badge/CUDA-12.1-76B900?style=for-the-badge&logo=nvidia&logoColor=white)
![License](https://img.shields.io/badge/License-MIT-FF6B2B?style=for-the-badge)
![Status](https://img.shields.io/badge/Status-Hackathon_Submission-22C55E?style=for-the-badge)

<br>

<img src="assets/banner.png" alt="Project Banner" width="100%">

<br>

**Hybrid loss optimization for class-imbalanced terrain segmentation**
**in autonomous off-road navigation systems.**

<br>

[ğŸ“Š View Report](#-report) Â· [ğŸš€ Quick Start](#-quick-start) Â· [ğŸ® Launch UI](#-web-interface) Â· [ğŸ“ˆ Results](#-results) Â· [ğŸ§  Models](#-models-trained)

<br>

---
</div>

<br>

## ğŸ¯ Project Overview

<table>
<tr>
<td width="60%">

We built an end-to-end **semantic segmentation system** that identifies **10 terrain classes** in desert off-road environments for autonomous vehicle navigation.

The core challenge? **Extreme class imbalance.** Sky dominates ~40% of pixels while safety-critical obstacles like Logs represent just ~0.5%. Standard models go blind to what matters most.

**Our solution:** A hybrid **Cross-Entropy + Dice Loss** function that forces the model to detect small, dangerous obstacles with the same priority as large background regions.

We trained **4 models** in a systematic ablation study to isolate and quantify the impact of each optimization strategy.

</td>
<td width="40%">

```text
ğŸ“Š KEY RESULTS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Baseline mIoU â†’ XX.XX%
Final mIoU â†’ XX.XX%
Logs IoU Gain â†’ XX.XX%
Models Trained â†’ 4
Classes â†’ 10
Resolution â†’ 512Ã—512
```

</td>
</tr>
</table>

<br>

## âš¡ Tech Stack

<div align="center">

| Category | Technologies |
|:--------:|:------------|
| ğŸ§  **Core ML** | `PyTorch 2.x` Â· `segmentation-models-pytorch` Â· `CUDA 12.1` |
| ğŸ“¸ **Vision** | `OpenCV` Â· `Albumentations` Â· `Pillow` |
| ğŸ“Š **Analysis** | `NumPy` Â· `Matplotlib` Â· `Seaborn` Â· `scikit-learn` |
| ğŸŒ **Web UI** | `Gradio` Â· `FastAPI` (optional) |
| ğŸ› ï¸ **DevOps** | `Miniconda` Â· `Git` Â· `GitHub` |
| ğŸ—ï¸ **Architecture** | `U-Net` Â· `ResNet34 (ImageNet)` |
| ğŸ“‰ **Loss** | `CrossEntropy + DiceLoss (Hybrid)` |

</div>

<br>

## ğŸ“ Project Structure

```text
off-road-vision/
â”‚
â”œâ”€â”€ ğŸ§  model/
â”‚   â”œâ”€â”€ train.py          â† Training script (all 4 models)
â”‚   â”œâ”€â”€ test.py           â† Evaluation + per-class IoU
â”‚   â”œâ”€â”€ model.py          â† U-Net architecture definition
â”‚   â”œâ”€â”€ dataset.py        â† Custom dataset + map_mask()
â”‚   â”œâ”€â”€ losses.py         â† CE Loss, Dice Loss, Hybrid Loss
â”‚   â””â”€â”€ config.py         â† All hyperparameters
â”‚
â”œâ”€â”€ ğŸ® ui/
â”‚   â”œâ”€â”€ app.py            â† Gradio web interface
â”‚   â”œâ”€â”€ inference.py      â† Single-image prediction pipeline
â”‚   â””â”€â”€ utils.py          â† Visualization helpers
â”‚
â”œâ”€â”€ ğŸ“Š outputs/
â”‚   â”œâ”€â”€ predictions/      â† Model prediction visualizations
â”‚   â”œâ”€â”€ graphs/           â† Loss curves, IoU charts
â”‚   â””â”€â”€ failure_cases/    â† Documented failure examples
â”‚
â”œâ”€â”€ ğŸ’¾ weights/
â”‚   â”œâ”€â”€ model_a_baseline.pth â† CE only, no augmentation
â”‚   â”œâ”€â”€ model_b_ce_aug.pth   â† CE + augmentation
â”‚   â”œâ”€â”€ model_c_hybrid.pth   â† CE+Dice, no augmentation
â”‚   â””â”€â”€ model_d_final.pth    â† CE+Dice + augmentation â˜…
â”‚
â”œâ”€â”€ ğŸ“„ report/
â”‚   â”œâ”€â”€ report.pdf        â† Final hackathon report
â”‚   â””â”€â”€ assets/           â† Report images and figures
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ environment.yml
â””â”€â”€ README.md             â† You are here
```

<br>

## ğŸš€ Quick Start

### Prerequisites
```bash
# Make sure you have:
# âœ“ Python 3.10+
# âœ“ NVIDIA GPU with CUDA 12.1+ (recommended)
# âœ“ ~4GB free GPU memory
# âœ“ ~2GB disk space for dataset
```

**Option 1: Conda (Recommended)**
```bash
# 1. Clone the repository
git clone [https://github.com/](https://github.com/)[your-username]/off-road-vision.git
cd off-road-vision

# 2. Create conda environment
conda create -n offroad python=3.10 -y
conda activate offroad

# 3. Install PyTorch with CUDA
pip install torch torchvision torchaudio \
    --index-url [https://download.pytorch.org/whl/cu121](https://download.pytorch.org/whl/cu121)

# 4. Install all dependencies
pip install -r requirements.txt

# 5. Verify GPU access
python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}')"
```

**Option 2: pip only**
```bash
git clone [https://github.com/](https://github.com/)[your-username]/off-road-vision.git
cd off-road-vision
pip install -r requirements.txt
```

<br>

## ğŸ§  Models Trained

We trained **4 models** in a systematic **2Ã—2 ablation study** to isolate contributions:

<div align="center">

```text
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚         AUGMENTATION                    â”‚
                    â”‚     None          H+V Flip              â”‚
                    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
          CE Only   â”‚   MODEL A    â”‚      MODEL B             â”‚
LOSS                â”‚   Baseline   â”‚   + Augmentation         â”‚
FUNCTION            â”‚   ğŸ”´         â”‚   ğŸŸ                      â”‚
                    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
          CE+Dice   â”‚   MODEL C    â”‚      MODEL D  â˜…          â”‚
          (Hybrid)  â”‚   + Dice     â”‚   + Both (FINAL)         â”‚
                    â”‚   ğŸ”µ         â”‚   ğŸŸ¢                     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

</div>

<br>

| Model | Loss Function | Augmentation | mIoU | Logs IoU | Status |
|:---|:---|:---|:---|:---|:---|
| ğŸ”´ **A** | CE Only | None | XX.XX% | XX.XX% | Baseline |
| ğŸŸ  **B** | CE Only | H+V Flip | XX.XX% | XX.XX% | + Aug |
| ğŸ”µ **C** | CE + Dice | None | XX.XX% | XX.XX% | + Loss |
| ğŸŸ¢ **D** | CE + Dice | H+V Flip | XX.XX% | XX.XX% | Final â˜… |

<br>

## ğŸ‹ï¸ Training

**Train All 4 Models**
```bash
# Train Model A â€” Baseline (CE only, no augmentation)
python model/train.py --model a --loss ce --augment none

# Train Model B â€” CE + Augmentation
python model/train.py --model b --loss ce --augment flip

# Train Model C â€” Hybrid Loss, no augmentation
python model/train.py --model c --loss hybrid --augment none

# Train Model D â€” Hybrid Loss + Augmentation (Final)
python model/train.py --model d --loss hybrid --augment flip
```

**Train with Custom Config**
```bash
python model/train.py \
    --model d \
    --loss hybrid \
    --augment flip \
    --epochs 15 \
    --batch-size 6 \
    --lr 0.0001 \
    --input-size 512 \
    --seed 42
```

**Training Configuration**
```yaml
# config.py â€” All hyperparameters
ARCHITECTURE:     U-Net + ResNet34
ENCODER_WEIGHTS:  imagenet
OPTIMIZER:        Adam (Î²1=0.9, Î²2=0.999, Îµ=1e-8)
LEARNING_RATE:    1e-4
LR_SCHEDULER:     CosineAnnealingLR (T_max=15)
LOSS_FUNCTION:    CrossEntropy + DiceLoss (Hybrid)
EPOCHS:           15
BATCH_SIZE:       6
INPUT_SIZE:       512 Ã— 512
NUM_CLASSES:      10
RANDOM_SEED:      42
CHECKPOINT:       Min Validation Loss
```

<br>

## ğŸ“Š Evaluation

**Evaluate Any Model**
```bash
# Evaluate Model D (final submission)
python model/test.py --weights weights/model_d_final.pth

# Evaluate all 4 models and compare
python model/test.py --compare-all

# Generate per-class IoU breakdown
python model/test.py --weights weights/model_d_final.pth --detailed
```

**Expected Output**
```text
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘           MODEL D â€” EVALUATION RESULTS               â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                      â•‘
â•‘  Class            IoU        Status                  â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€               â•‘
â•‘  Sky              XX.XX%     âœ… Strong               â•‘
â•‘  Trees            XX.XX%     âœ… Strong               â•‘
â•‘  Lush Bushes      XX.XX%     âœ… Improved             â•‘
â•‘  Landscape        XX.XX%     âœ… Improved             â•‘
â•‘  Rocks            XX.XX%     âœ… Improved             â•‘
â•‘  Logs â­          XX.XX%     ğŸ† Critical Win         â•‘
â•‘  Dry Bushes       XX.XX%     âœ… Improved             â•‘
â•‘  Gravel Path      XX.XX%     âœ… Improved             â•‘
â•‘  Sand             XX.XX%     âœ… Improved             â•‘
â•‘  Dry Grass        XX.XX%     âœ… Improved             â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€               â•‘
â•‘  Mean IoU         XX.XX%     ğŸ† Final Score          â•‘
â•‘  Pixel Accuracy   XX.XX%                             â•‘
â•‘                                                      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

<br>

## ğŸ® Web Interface

We built a **Gradio-powered web UI** for real-time terrain segmentation inference.

**Launch the UI**
```bash
# Start the web interface
python ui/app.py

# Or specify a custom port
python ui/app.py --port 7860 --share
```

**What You Can Do**
```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ® OFF-ROAD VISION â€” WEB INTERFACE                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  ğŸ“¤ UPLOAD           Drop any desert terrain image      â”‚
â”‚                      (supports JPG, PNG, BMP)           â”‚
â”‚                                                         â”‚
â”‚  ğŸ§  SELECT MODEL     Choose from all 4 trained models   â”‚
â”‚                      Model A / B / C / D                â”‚
â”‚                                                         â”‚
â”‚  ğŸ¯ SEGMENT          One-click semantic segmentation    â”‚
â”‚                      Real-time inference on GPU         â”‚
â”‚                                                         â”‚
â”‚  ğŸ“Š VIEW RESULTS     Side-by-side comparison:           â”‚
â”‚                      Original â†’ Overlay â†’ Class Mask    â”‚
â”‚                                                         â”‚
â”‚  ğŸ“‹ CLASS LEGEND     Color-coded terrain class labels   â”‚
â”‚                      with confidence percentages        â”‚
â”‚                                                         â”‚
â”‚  ğŸ’¾ DOWNLOAD         Save prediction mask as PNG        â”‚
â”‚                                                         â”‚
â”‚  ğŸ”„ COMPARE          Run same image through all 4       â”‚
â”‚                      models side-by-side                â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**UI Preview**
```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸœï¸ Off-Road Autonomous Vision                                  â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                 â”‚  â”‚                 â”‚  â”‚                â”‚   â”‚
â”‚  â”‚   ğŸ“¤ Upload     â”‚  â”‚  ğŸ¯ Segmented   â”‚  â”‚ ğŸ—ºï¸ Class Mask  â”‚   â”‚
â”‚  â”‚   Image Here    â”‚  â”‚  Overlay        â”‚  â”‚                â”‚   â”‚
â”‚  â”‚                 â”‚  â”‚                 â”‚  â”‚                â”‚   â”‚
â”‚  â”‚  [DROP IMAGE]   â”‚  â”‚  [PREDICTION]   â”‚  â”‚  [COLOR MAP]   â”‚   â”‚
â”‚  â”‚                 â”‚  â”‚                 â”‚  â”‚                â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                 â”‚
â”‚  Model: [Model D â˜… â–¼]    Resolution: 512Ã—512                    â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  CLASS LEGEND                                           â”‚    â”‚
â”‚  â”‚  ğŸŸ¦ Sky  ğŸŸ© Trees  ğŸŸ« Rocks  ğŸŸ§ Logs  ğŸŸ¨ Sand  ...      â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                 â”‚
â”‚  [ ğŸ¯ Segment ]  [ ğŸ”„ Compare All Models ]  [ ğŸ’¾ Download ]     â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Gradio App Code (`ui/app.py`)**
```python
"""
Off-Road Autonomous Vision â€” Web Interface
Gradio-powered real-time terrain segmentation
"""
import gradio as gr
import torch
import numpy as np
import cv2
from inference import SegmentationInference

# â”€â”€â”€ Initialize Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
engine = SegmentationInference(
    weights_path="weights/model_d_final.pth",
    device="cuda" if torch.cuda.is_available() else "cpu",
    input_size=512,
    num_classes=10
)

# â”€â”€â”€ Class Color Map â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CLASS_NAMES = [
    "Sky", "Trees", "Lush Bushes", "Landscape", "Rocks",
    "Logs", "Dry Bushes", "Gravel Path", "Sand", "Dry Grass"
]

CLASS_COLORS = [
    [135, 206, 235],  # Sky - light blue
    [34, 139, 34],    # Trees - forest green
    [0, 128, 0],      # Lush Bushes - green
    [210, 180, 140],  # Landscape - tan
    [128, 128, 128],  # Rocks - gray
    [139, 69, 19],    # Logs - brown
    [189, 183, 107],  # Dry Bushes - khaki
    [169, 169, 169],  # Gravel Path - dark gray
    [244, 164, 96],   # Sand - sandy brown
    [154, 205, 50],   # Dry Grass - yellow green
]

# â”€â”€â”€ Prediction Function â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def predict(image, model_choice):
    """Run segmentation on uploaded image."""
    
    # Select model weights
    weight_map = {
        "Model A â€” Baseline (CE Only)": "weights/model_a_baseline.pth",
        "Model B â€” CE + Augmentation": "weights/model_b_ce_aug.pth",
        "Model C â€” Hybrid Loss": "weights/model_c_hybrid.pth",
        "Model D â€” Final (Hybrid + Aug) â˜…": "weights/model_d_final.pth",
    }
    
    engine.load_weights(weight_map[model_choice])
    
    # Run inference
    mask = engine.predict(image)
    
    # Create colored overlay
    overlay = engine.create_overlay(image, mask, alpha=0.5)
    
    # Create class mask visualization
    color_mask = engine.create_color_mask(mask, CLASS_COLORS)
    
    # Generate class distribution text
    stats = engine.get_class_stats(mask, CLASS_NAMES)
    
    return overlay, color_mask, stats

# â”€â”€â”€ Compare All Models â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def compare_all(image):
    """Run same image through all 4 models."""
    results = []
    for weight_file in [
        "weights/model_a_baseline.pth",
        "weights/model_b_ce_aug.pth",
        "weights/model_c_hybrid.pth",
        "weights/model_d_final.pth"
    ]:
        engine.load_weights(weight_file)
        mask = engine.predict(image)
        overlay = engine.create_overlay(image, mask, alpha=0.5)
        results.append(overlay)
        
    return results[0], results[1], results[2], results[3]

# â”€â”€â”€ Build Gradio Interface â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with gr.Blocks(
    title="Off-Road Autonomous Vision",
    theme=gr.themes.Base(
        primary_hue="orange",
        secondary_hue="blue",
        neutral_hue="slate",
        font=gr.themes.GoogleFont("Inter"),
    ),
    css="""
    .gradio-container {
        max-width: 1200px !important;
    }
    .header {
        text-align: center;
        padding: 20px;
        background: linear-gradient(135deg, #0A1628, #162033);
        border-radius: 12px;
        margin-bottom: 20px;
    }
    .header h1 {
        color: #FF6B2B;
        font-size: 2em;
    }
    .header p {
        color: #94A3B8;
    }
    """
) as demo:
    
    # â”€â”€ Header â”€â”€
    gr.HTML("""
    <div class="header">
        <h1>ğŸœï¸ Off-Road Autonomous Vision</h1>
        <p>Real-time semantic segmentation for desert terrain navigation</p>
        <p style="color: #FF6B2B; font-size: 0.9em;">
            U-Net + ResNet34  Â·  Hybrid CE + Dice Loss  Â·  10 Terrain Classes  Â·  512Ã—512
        </p>
    </div>
    """)
    
    # â”€â”€ Single Model Tab â”€â”€
    with gr.Tab("ğŸ¯ Single Model Inference"):
        with gr.Row():
            with gr.Column(scale=1):
                input_image = gr.Image(
                    label="ğŸ“¤ Upload Terrain Image",
                    type="numpy",
                    height=400
                )
                model_dropdown = gr.Dropdown(
                    choices=[
                        "Model A â€” Baseline (CE Only)",
                        "Model B â€” CE + Augmentation",
                        "Model C â€” Hybrid Loss",
                        "Model D â€” Final (Hybrid + Aug) â˜…",
                    ],
                    value="Model D â€” Final (Hybrid + Aug) â˜…",
                    label="ğŸ§  Select Model"
                )
                segment_btn = gr.Button(
                    "ğŸ¯ Segment Terrain",
                    variant="primary",
                    size="lg"
                )
                
            with gr.Column(scale=1):
                output_overlay = gr.Image(
                    label="ğŸ¯ Segmented Overlay",
                    height=400
                )
                
            with gr.Column(scale=1):
                output_mask = gr.Image(
                    label="ğŸ—ºï¸ Class Mask",
                    height=400
                )
                output_stats = gr.Textbox(
                    label="ğŸ“Š Class Distribution",
                    lines=12
                )
                
        segment_btn.click(
            fn=predict,
            inputs=[input_image, model_dropdown],
            outputs=[output_overlay, output_mask, output_stats]
        )

    # â”€â”€ Compare All Models Tab â”€â”€
    with gr.Tab("ğŸ”„ Compare All Models"):
        with gr.Row():
            compare_input = gr.Image(
                label="ğŸ“¤ Upload Image",
                type="numpy",
                height=300
            )
            compare_btn = gr.Button(
                "ğŸ”„ Compare All 4 Models",
                variant="primary",
                size="lg"
            )
            
        with gr.Row():
            out_a = gr.Image(label="ğŸ”´ Model A â€” Baseline")
            out_b = gr.Image(label="ğŸŸ  Model B â€” + Aug")
            out_c = gr.Image(label="ğŸ”µ Model C â€” + Dice")
            out_d = gr.Image(label="ğŸŸ¢ Model D â€” Final â˜…")
            
        compare_btn.click(
            fn=compare_all,
            inputs=[compare_input],
            outputs=[out_a, out_b, out_c, out_d]
        )

    # â”€â”€ Class Legend Tab â”€â”€
    with gr.Tab("ğŸ“‹ Class Legend"):
        gr.HTML("""
        <div style="padding: 20px; background: #0A1628; border-radius: 12px;">
            <h3 style="color: #FF6B2B;">10 Terrain Classes</h3>
            <table style="width: 100%; color: white; border-collapse: collapse;">
                <tr style="border-bottom: 1px solid #1E293B;">
                    <th style="padding: 8px;">ID</th>
                    <th>Color</th>
                    <th>Class Name</th>
                    <th>Category</th>
                </tr>
                <tr><td>0</td><td>ğŸŸ¦</td><td>Sky</td><td>Background</td></tr>
                <tr><td>1</td><td>ğŸŸ©</td><td>Trees</td><td>Vegetation</td></tr>
                <tr><td>2</td><td>ğŸŒ¿</td><td>Lush Bushes</td><td>Vegetation</td></tr>
                <tr><td>3</td><td>ğŸŸ«</td><td>Landscape</td><td>Background</td></tr>
                <tr><td>4</td><td>â¬œ</td><td>Rocks</td><td>Obstacle</td></tr>
                <tr><td>5</td><td>ğŸŸ«</td><td>Logs</td><td>Obstacle âš ï¸</td></tr>
                <tr><td>6</td><td>ğŸŸ¨</td><td>Dry Bushes</td><td>Vegetation</td></tr>
                <tr><td>7</td><td>â¬›</td><td>Gravel Path</td><td>Navigable</td></tr>
                <tr><td>8</td><td>ğŸŸ§</td><td>Sand</td><td>Navigable</td></tr>
                <tr><td>9</td><td>ğŸŸ¡</td><td>Dry Grass</td><td>Vegetation</td></tr>
            </table>
        </div>
        """)

# â”€â”€â”€ Launch â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        show_error=True
    )
```

<br>

## ğŸ“ˆ Results

**Performance Progression**
```text
mIoU Performance Across 4 Models
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Model A â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  XX.XX%
Model B â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  XX.XX%
Model C â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  XX.XX%
Model D â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  XX.XX%  â˜…
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                                     Target: Maximum mIoU
```

**Ablation Analysis**
```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                         â”‚
â”‚  WHAT CONTRIBUTED MORE?                                 â”‚
â”‚                                                         â”‚
â”‚  Augmentation alone (Aâ†’B):      +XX.XX% mIoU            â”‚
â”‚  Hybrid Loss alone  (Aâ†’C):      +XX.XX% mIoU            â”‚
â”‚  Combined effect    (Aâ†’D):      +XX.XX% mIoU    â˜…       â”‚
â”‚                                                         â”‚
â”‚  â†’ [Loss function / Augmentation] was the primary       â”‚
â”‚    performance driver for minority class recovery       â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Critical Safety Metric**
```text
Logs IoU (Most Dangerous Obstacle Class)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Model A  â–“â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  XX.XX%   Invisible
Model B  â–“â–“â–“â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  XX.XX%   Partial
Model C  â–“â–“â–“â–“â–“â–“â–“â–“â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  XX.XX%   Detected
Model D  â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  XX.XX%   Reliable â˜…
```

<br>

## ğŸ” Reproducing Results

**Full Reproduction Pipeline**
```bash
# Step 1: Environment
conda create -n offroad python=3.10 -y
conda activate offroad
pip install -r requirements.txt

# Step 2: Dataset
# Place dataset in project root:
# Offroad_Segmentation_Training_Dataset/
# â”œâ”€â”€ images/
# â””â”€â”€ masks/

# Step 3: Train all 4 models
python model/train.py --model a --loss ce --augment none
python model/train.py --model b --loss ce --augment flip
python model/train.py --model c --loss hybrid --augment none
python model/train.py --model d --loss hybrid --augment flip

# Step 4: Evaluate
python model/test.py --compare-all

# Step 5: Generate report visuals
python generate_report_visuals.py

# Step 6: Launch UI
python ui/app.py
```

**Verify Your Results Match Ours**
```bash
python model/test.py --weights weights/model_d_final.pth --detailed
# Expected output should show:
# Mean IoU:     ~XX.XX%  (Â± 0.5%)
# Pixel Acc:    ~XX.XX%  (Â± 0.3%)
# Logs IoU:     ~XX.XX%  (Â± 1.0%)
```

<br>

## âš™ï¸ Requirements

**`requirements.txt`**
```text
torch>=2.0.0
torchvision>=0.15.0
torchaudio>=2.0.0
segmentation-models-pytorch>=0.3.3
albumentations>=1.3.1
opencv-python>=4.8.0
numpy>=1.24.0
matplotlib>=3.7.0
seaborn>=0.12.0
scikit-learn>=1.3.0
Pillow>=10.0.0
gradio>=4.0.0
tqdm>=4.65.0
```

**`environment.yml`**
```yaml
name: offroad
channels:
  - pytorch
  - nvidia
  - defaults
dependencies:
  - python=3.10
  - pip
  - pip:
    - torch>=2.0.0
    - torchvision>=0.15.0
    - segmentation-models-pytorch>=0.3.3
    - albumentations>=1.3.1
    - opencv-python>=4.8.0
    - numpy>=1.24.0
    - matplotlib>=3.7.0
    - seaborn>=0.12.0
    - scikit-learn>=1.3.0
    - gradio>=4.0.0
    - tqdm>=4.65.0
```

<br>

## ğŸ—ºï¸ Class Definitions

| ID | Class | Pixel Freq | Category | Safety Level |
|:---|:---|:---|:---|:---|
| 0 | Sky | ~40.2% | Background | ğŸŸ¢ None |
| 1 | Trees | ~15.3% | Vegetation | ğŸŸ¡ Low |
| 2 | Lush Bushes | ~X.X% | Vegetation | ğŸŸ¡ Low |
| 3 | Landscape | ~22.1% | Background | ğŸŸ¢ None |
| 4 | Rocks | ~1.2% | Obstacle | ğŸ”´ High |
| 5 | Logs | ~0.5% | Obstacle | ğŸ”´ Critical |
| 6 | Dry Bushes | ~X.X% | Vegetation | ğŸŸ¡ Low |
| 7 | Gravel Path | ~X.X% | Navigable | ğŸŸ¢ None |
| 8 | Sand | ~X.X% | Navigable | ğŸŸ¢ None |
| 9 | Dry Grass | ~X.X% | Vegetation | ğŸŸ¡ Low |

<br>

## âš ï¸ Known Limitations

| Failure Mode | Description | Severity | Proposed Fix |
|:---|:---|:---|:---|
| Shadow â†’ Rock | Shadows misclassified as rocks | Medium | LiDAR depth fusion |
| Grass â†” Bush | Boundary bleeding at transitions | Low | Boundary loss terms |
| Low light | Reduced accuracy in dark images | Medium | PhotoAugmentation |

<br>

## ğŸ“„ Report

ğŸ“ **View Full Hackathon Report (PDF)**

The report covers:
* 4-model ablation study with complete comparison tables
* Per-class IoU analysis across all models
* Loss curve analysis and convergence behavior
* Failure mode documentation with visual evidence
* Future work recommendations

<br>

## ğŸ™ Acknowledgments

* **segmentation-models-pytorch** â€” Pre-built architectures
* **Albumentations** â€” Fast image augmentation
* **Gradio** â€” Web interface framework
* **PyTorch** â€” Deep learning framework

<br>

<div align="center">
Built with ğŸ§  and â˜• for [HACKATHON NAME] 2025<br>
[TEAM NAME]
</div>
